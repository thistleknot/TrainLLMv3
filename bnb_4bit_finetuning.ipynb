{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f91de3ed-d5d1-4c62-b54a-f5af04e54bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-06-03 08:13:42.848829: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda115_nocublaslt.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 115\n",
      "CUDA SETUP: Loading binary /mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda115_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-I/root')}\n",
      "  warn(msg)\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('2')}\n",
      "  warn(msg)\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('-L/root/boost_1_75_0/stage/lib')}\n",
      "  warn(msg)\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/var/lib/snap')}\n",
      "  warn(msg)\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('bl=^G'), PosixPath('li#41'), PosixPath('DC=\\\\E[%dP'), PosixPath('mh=\\\\E[2m'), PosixPath('kB=\\\\E[Z'), PosixPath('up=\\\\EM'), PosixPath('kR=\\\\E[1;2A'), PosixPath('rs=\\\\Ec'), PosixPath('k2=\\\\EOQ'), PosixPath('#3=\\\\E[2;2~'), PosixPath('am'), PosixPath('st=\\\\EH'), PosixPath('md=\\\\E[1m'), PosixPath('kh=\\\\E[1~'), PosixPath('F7=\\\\E[15;2~'), PosixPath('se=\\\\E[23m'), PosixPath('pf=\\\\E[4i'), PosixPath('km'), PosixPath('is=\\\\E)0'), PosixPath('kH=\\\\E[4~'), PosixPath('dl=\\\\E[M'), PosixPath('as=\\\\E(0'), PosixPath('xn'), PosixPath('cl=\\\\E[H\\\\E[J'), PosixPath('ue=\\\\E[24m'), PosixPath('kb=\\x7f'), PosixPath('k7=\\\\E[18~'), PosixPath('cr=^M'), PosixPath('LE=\\\\E[%dD'), PosixPath('F4=\\\\E[1;2Q'), PosixPath('k1=\\\\EOP'), PosixPath('pa#64'), PosixPath('\\\\\\n\\t'), PosixPath('IC=\\\\E[%d@'), PosixPath('k9=\\\\E[20~'), PosixPath('pt'), PosixPath('%i=\\\\E[1;2C'), PosixPath('cd=\\\\E[J'), PosixPath('cs=\\\\E[%i%d;%dr'), PosixPath('%e=\\\\E[5;2~'), PosixPath('vs=\\\\E[34l'), PosixPath('dc=\\\\E[P'), PosixPath('*4=\\\\E[3;2~'), PosixPath('SC|screen|VT 100/ANSI X3.64 virtual terminal'), PosixPath('Km=\\\\E[M'), PosixPath('ks=\\\\E[?1h\\\\E='), PosixPath('te=\\\\E[?1049l'), PosixPath('ta=^I'), PosixPath('ce=\\\\E[K'), PosixPath('UP=\\\\E[%dA'), PosixPath('kl=\\\\EOD'), PosixPath('DL=\\\\E[%dM'), PosixPath('AF=\\\\E[3%dm'), PosixPath('AB=\\\\E[4%dm'), PosixPath('k3=\\\\EOR'), PosixPath('F3=\\\\E[1;2P'), PosixPath('FD=\\\\E[23;2~'), PosixPath('F2=\\\\E[24~'), PosixPath('vi=\\\\E[?25l'), PosixPath('do=^J'), PosixPath('@7=\\\\E[4~'), PosixPath('im=\\\\E[4h'), PosixPath('bs'), PosixPath('rc=\\\\E8'), PosixPath('ei=\\\\E[4l'), PosixPath('ve=\\\\E[34h\\\\E[?25h'), PosixPath('kN=\\\\E[6~'), PosixPath('me=\\\\E[m'), PosixPath('k4=\\\\EOS'), PosixPath('#2=\\\\E[1;2H'), PosixPath('nw=\\\\EE'), PosixPath('kD=\\\\E[3~'), PosixPath('po=\\\\E[5i'), PosixPath('DO=\\\\E[%dB'), PosixPath('RI=\\\\E[%dC'), PosixPath('k6=\\\\E[17~'), PosixPath('%c=\\\\E[6;2~'), PosixPath('ms'), PosixPath('bt=\\\\E[Z'), PosixPath('us=\\\\E[4m'), PosixPath('K2=\\\\EOE'), PosixPath('le=^H'), PosixPath('nd=\\\\E[C'), PosixPath('cm=\\\\E[%i%d;%dH'), PosixPath('it#8'), PosixPath('LP'), PosixPath('co#155'), PosixPath('op=\\\\E[39;49m'), PosixPath('AX'), PosixPath('mb=\\\\E[5m'), PosixPath('k8=\\\\E[19~'), PosixPath('kd=\\\\EOB'), PosixPath('kr=\\\\EOC'), PosixPath('F6=\\\\E[1;2S'), PosixPath('ct=\\\\E[3g'), PosixPath('so=\\\\E[3m'), PosixPath('FA=\\\\E[19;2~'), PosixPath('ac=\\\\140\\\\140aaffggjjkkllmmnnooppqqrrssttuuvvwwxxyyzz{{||}}~~..--++,,hhII00'), PosixPath('FC=\\\\E[21;2~'), PosixPath('kP=\\\\E[5~'), PosixPath('vb=\\\\Eg'), PosixPath('mr=\\\\E[7m'), PosixPath('Co#8'), PosixPath('ho=\\\\E[H'), PosixPath('k5=\\\\E[15~'), PosixPath('k;=\\\\E[21~'), PosixPath('xv'), PosixPath('*7=\\\\E[1;2F'), PosixPath('#4=\\\\E[1;2D'), PosixPath('ku=\\\\EOA'), PosixPath('FE=\\\\E[24;2~'), PosixPath('sr=\\\\EM'), PosixPath('mi'), PosixPath('@1=\\\\E[1~'), PosixPath('F8=\\\\E[17;2~'), PosixPath('al=\\\\E[L'), PosixPath('sc=\\\\E7'), PosixPath('ae=\\\\E(B'), PosixPath('F1=\\\\E[23~'), PosixPath('kF=\\\\E[1;2B'), PosixPath('AL=\\\\E[%dL'), PosixPath('F5=\\\\E[1;2R'), PosixPath('kI=\\\\E[2~'), PosixPath('FB=\\\\E[20;2~'), PosixPath('ke=\\\\E[?1l\\\\E>'), PosixPath('k0=\\\\E[10~'), PosixPath('F9=\\\\E[18;2~'), PosixPath('ti=\\\\E[?1049h'), PosixPath('G0')}\n",
      "  warn(msg)\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  ( alias;\\n eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot \"$@\"\\n}')}\n",
      "  warn(msg)\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  unset _mlshdbg;\\n if [ \"${MODULES_SILENT_SHELL_DEBUG'), PosixPath('-}${_mlv}_modquar=\\'`eval \\'echo ${\\'$_mlv\\'}\\'`\\' \";\\n fi;\\n _mlrv=\"MODULES_RUNENV_${_mlv}\";\\n _mlre=\"${_mlre'), PosixPath('-}\\'`\\' \";\\n fi;\\n done;\\n if [ -n \"${_mlre'), PosixPath('-};\\n do\\n if [ \"${_mlv}\" = \"${_mlv##*[!A-Za-z0-9_]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\\n if [ -n \"`eval \\'echo ${\\'$_mlv\\'+x}\\'`\" ]; then\\n _mlre=\"${_mlre'), PosixPath(\"-}${_mlv}='`eval 'echo ${'$_mlrv'\"), PosixPath('-0}\" = \\'1\\' ]; then\\n case \"$-\" in \\n *v*x*)\\n set +vx;\\n _mlshdbg=\\'vx\\'\\n ;;\\n *v*)\\n set +v;\\n _mlshdbg=\\'v\\'\\n ;;\\n *x*)\\n set +x;\\n _mlshdbg=\\'x\\'\\n ;;\\n *)\\n _mlshdbg=\\'\\'\\n ;;\\n esac;\\n fi;\\n unset _mlre _mlIFS;\\n if [ -n \"${IFS+x}\" ]; then\\n _mlIFS=$IFS;\\n fi;\\n IFS=\\' \\';\\n for _mlv in ${MODULES_RUN_QUARANTINE'), PosixPath('-}\" ]; then\\n eval `eval ${_mlre} /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \\'\"$@\"\\'`;\\n else\\n eval `/usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \"$@\"`;\\n fi;\\n _mlstatus=$?;\\n if [ -n \"${_mlIFS+x}\" ]; then\\n IFS=$_mlIFS;\\n else\\n unset IFS;\\n fi;\\n unset _mlre _mlv _mlrv _mlIFS;\\n if [ -n \"${_mlshdbg'), PosixPath('-}\" ]; then\\n set -$_mlshdbg;\\n fi;\\n unset _mlshdbg;\\n return $_mlstatus\\n}')}\n",
      "  warn(msg)\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('() {  typeset swfound=1;\\n if [ \"${MODULES_USE_COMPAT_VERSION'), PosixPath('-0}\" = \\'1\\' ]; then\\n typeset swname=\\'main\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\\n typeset swfound=0;\\n unset MODULES_USE_COMPAT_VERSION;\\n fi;\\n else\\n typeset swname=\\'compatibility\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\\n typeset swfound=0;\\n MODULES_USE_COMPAT_VERSION=1;\\n export MODULES_USE_COMPAT_VERSION;\\n fi;\\n fi;\\n if [ $swfound -eq 0 ]; then\\n echo \"Switching to Modules $swname version\";\\n source /usr/share/Modules/init/bash;\\n else\\n echo \"Cannot switch to Modules $swname version, command not found\";\\n return 1;\\n fi\\n}')}\n",
      "  warn(msg)\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from common_imports import *\n",
    "from functions import (\n",
    "    create_subset,\n",
    "    filter_datasets_for_use_case,\n",
    "    split_datasets,\n",
    "    unique_elements,\n",
    "    PerplexityLoggingCallback,\n",
    "    print_trainable_parameters,\n",
    "    CustomDataset,\n",
    "    get_sequences,\n",
    "    evaluate,\n",
    "    CustomTrainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517260a2-a489-4118-b7cc-f98b35fa8454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed5da9-cff8-4ae8-b27f-16df0a16cb81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe816b4-6c1f-407a-92de-175873789fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb5966da-98d0-4321-bff0-923509d368cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndevice_map = {\\n    \"transformer.word_embeddings\": 0,\\n    \"transformer.word_embeddings_layernorm\": 0,\\n    \"lm_head\": \"cpu\",\\n    \"transformer.h\": 0,\\n    \"transformer.ln_f\": 0,\\n    #\"llm_int8_enable_fp32_cpu_offload\": True\\n}\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open(\"resources/train.json\", \"r\") as f:\n",
    "    args = json.load(f)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=args[\"load_in_4bit\"],\n",
    "    bnb_4bit_use_double_quant=args[\"bnb_4bit_use_double_quant\"],\n",
    "    bnb_4bit_quant_type=args[\"bnb_4bit_quant_type\"],\n",
    "    bnb_4bit_compute_dtype=eval(args[\"bnb_4bit_compute_dtype\"]),\n",
    "    #llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=args[\"lora_r\"],\n",
    "    lora_alpha=args[\"lora_alpha\"],\n",
    "    lora_dropout=args[\"lora_dropout\"],\n",
    "    bias=args[\"lora_bias\"],\n",
    "    task_type=args[\"lora_task_type\"]\n",
    ")\n",
    "\n",
    "device_map = {\"lm_head\": \"cpu\",\"\": 0}\n",
    "\"\"\"\n",
    "device_map = {\n",
    "    \"transformer.word_embeddings\": 0,\n",
    "    \"transformer.word_embeddings_layernorm\": 0,\n",
    "    \"lm_head\": \"cpu\",\n",
    "    \"transformer.h\": 0,\n",
    "    \"transformer.ln_f\": 0,\n",
    "    #\"llm_int8_enable_fp32_cpu_offload\": True\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b886fe30-ffbe-4c7d-b7ce-7b0feacdfbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>peft_config = PeftConfig.from_pretrained(<span style=\"color: #808000; text-decoration-color: #808000\">'bits'</span>)                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 5 model = AutoModelForCausalLM.from_pretrained(                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>peft_config.base_model_name_or_path,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>quantization_config=bnb_config, device_map=device_map,                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span>)                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/transformers/models/auto/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">auto_factory.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">90</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">487 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">488 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(config) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>._model_mapping.keys():                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">489 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>model_class = _get_model_class(config, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>._model_mapping)                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>490 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> model_class.from_pretrained(                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">491 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">492 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">493 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2665</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2662 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>keys_on_cpu = [key <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> key, value <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> device_map.items() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> value <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> [<span style=\"color: #808000; text-decoration-color: #808000\">\"di</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2663 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2664 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(keys_on_cpu) &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> load_in_8bit_fp32_cpu_offload:            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2665 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2666 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"If you want to offload some keys to `cpu` or `disk`, you need t</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2667 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"`llm_int8_enable_fp32_cpu_offload=True`. Note that these module</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2668 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\" converted to 8-bit but kept in 32-bit.\"</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>If you want to offload some keys to `cpu` or `disk`, you need to set \n",
       "`<span style=\"color: #808000; text-decoration-color: #808000\">llm_int8_enable_fp32_cpu_offload</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>`. Note that these modules will not be  converted to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>-bit but kept in \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>-bit.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m5\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 2 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3 \u001b[0mpeft_config = PeftConfig.from_pretrained(\u001b[33m'\u001b[0m\u001b[33mbits\u001b[0m\u001b[33m'\u001b[0m)                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 5 model = AutoModelForCausalLM.from_pretrained(                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m│   │   \u001b[0mpeft_config.base_model_name_or_path,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m\u001b[2m│   │   \u001b[0mquantization_config=bnb_config, device_map=device_map,                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m)                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/transformers/models/auto/\u001b[0m\u001b[1;33mauto_factory.py\u001b[0m:\u001b[94m4\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m90\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m487 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m488 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mtype\u001b[0m(config) \u001b[95min\u001b[0m \u001b[96mcls\u001b[0m._model_mapping.keys():                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m489 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel_class = _get_model_class(config, \u001b[96mcls\u001b[0m._model_mapping)                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m490 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m model_class.from_pretrained(                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m491 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mpretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m492 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m493 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/transformers/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m2665\u001b[0m in     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mfrom_pretrained\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2662 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mkeys_on_cpu = [key \u001b[94mfor\u001b[0m key, value \u001b[95min\u001b[0m device_map.items() \u001b[94mif\u001b[0m value \u001b[95min\u001b[0m [\u001b[33m\"\u001b[0m\u001b[33mdi\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2663 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2664 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(keys_on_cpu) > \u001b[94m0\u001b[0m \u001b[95mand\u001b[0m \u001b[95mnot\u001b[0m load_in_8bit_fp32_cpu_offload:            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2665 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2666 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mIf you want to offload some keys to `cpu` or `disk`, you need t\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2667 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m`llm_int8_enable_fp32_cpu_offload=True`. Note that these module\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2668 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m converted to 8-bit but kept in 32-bit.\u001b[0m\u001b[33m\"\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0mIf you want to offload some keys to `cpu` or `disk`, you need to set \n",
       "`\u001b[33mllm_int8_enable_fp32_cpu_offload\u001b[0m=\u001b[3;92mTrue\u001b[0m`. Note that these modules will not be  converted to \u001b[1;36m8\u001b[0m-bit but kept in \n",
       "\u001b[1;36m32\u001b[0m-bit.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args['model_id'])\n",
    "\n",
    "peft_config = PeftConfig.from_pretrained('bits')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        peft_config.base_model_name_or_path,\n",
    "        quantization_config=bnb_config, device_map=device_map,\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "#peft wrapper feature\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6642d92-6546-4bf2-a536-09ccecdb76f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "440c7f44-9a1e-4bab-92da-10d0dcedfe7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16261 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before load\n",
      "160\n",
      "159\n"
     ]
    }
   ],
   "source": [
    "print(\"before load\")\n",
    "with open('../venv_train_neo/datasets_dict.pkl', 'rb') as f:\n",
    "    datasets_dict = pickle.load(f)\n",
    "    \n",
    "finetune_datasets = filter_datasets_for_use_case(datasets_dict, 'finetune')\n",
    "train_data_list, valid_data_list, valid_data_indices = split_datasets(finetune_datasets, ratio=args['split_ratio'], random_state=args['seed'])\n",
    "\n",
    "train_data_list = [record for dataset in train_data_list.values() for record in dataset]\n",
    "valid_data_list = [record for dataset in valid_data_list.values() for record in dataset]\n",
    "\n",
    "combined_train = tokenizer.eos_token.join(train_data_list)\n",
    "combined_valid = tokenizer.eos_token.join(valid_data_list)\n",
    "\n",
    "train_sequences = get_sequences(combined_train, tokenizer,seq_length=args['seq_length'])\n",
    "print(combined_train.count(tokenizer.eos_token)*2)\n",
    "print(np.sum([t.count(tokenizer.eos_token_id) for t in train_sequences]))\n",
    "\n",
    "valid_sequences = get_sequences(combined_valid, tokenizer, seq_length=args['seq_length'])\n",
    "\n",
    "train_epoch_steps = (len(train_sequences) / (args['batch_size'] * args['gradient_accumulation_steps']))\n",
    "valid_epoch_steps = (len(valid_sequences) / (args['batch_size'] * args['gradient_accumulation_steps']))\n",
    "\n",
    "max_train_steps = int(train_epoch_steps * args['epochs'])\n",
    "\n",
    "train_dataset = datasets.Dataset.from_dict({\"input_ids\": train_sequences})\n",
    "valid_dataset = datasets.Dataset.from_dict({\"input_ids\": valid_sequences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b283a-19ca-4f7d-9aa5-1abb290d878f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23e7c896-94a6-4c85-88ff-874794b0a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_valid_dataset = create_subset(valid_dataset, args['num_eval_examples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b824cc2-b148-4ab9-b605-9e4374369748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/mnt/distvol/bitsandbytes/lib/python3.9/site-packages/transformers/models/gpt_neo/modeling_gpt_neo.py:197: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:493.)\n",
      "  attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 01:28, Epoch 2/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.153800</td>\n",
       "      <td>4.290053</td>\n",
       "      <td>72.970328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.132400</td>\n",
       "      <td>4.288104</td>\n",
       "      <td>72.828225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.885600</td>\n",
       "      <td>4.287585</td>\n",
       "      <td>72.790486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.158000</td>\n",
       "      <td>4.287170</td>\n",
       "      <td>72.760295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.117000</td>\n",
       "      <td>4.285293</td>\n",
       "      <td>72.623830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.836000</td>\n",
       "      <td>4.285558</td>\n",
       "      <td>72.643052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.045100</td>\n",
       "      <td>4.285620</td>\n",
       "      <td>72.647590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7, training_loss=4.046854734420776, metrics={'train_runtime': 101.7385, 'train_samples_per_second': 6.605, 'train_steps_per_second': 0.069, 'total_flos': 21788643557376.0, 'train_loss': 4.046854734420776, 'epoch': 2.6})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#trainer = Trainer(\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=subset_valid_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size = args['batch_size'],\n",
    "        per_device_eval_batch_size = args['batch_size'],\n",
    "        gradient_accumulation_steps=args['gradient_accumulation_steps'],\n",
    "        warmup_steps=int(train_epoch_steps * args['warm_ratio']),\n",
    "        evaluation_strategy='steps',\n",
    "        max_steps=max_train_steps,\n",
    "        learning_rate=args['learning_rate']/10,\n",
    "        weight_decay=args['weight_decay'],      # Add weight decay argument\n",
    "        lr_scheduler_type='cosine',     # Add lr scheduler type argument\n",
    "        max_grad_norm=args['max_grad_norm'],       # Add max norm clipping argument\n",
    "        fp16=True,  # Add a keyword here\n",
    "        logging_steps=int(np.clip(np.round(train_epoch_steps/10),1,1)),\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    callbacks=[PerplexityLoggingCallback()],  # Add the custom callback\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8531a825-2070-4302-a9a4-f0edc2f96d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_completed_steps = trainer.get_completed_steps()\n",
    "\n",
    "valid_steps = max(1, int(np.round((initial_completed_steps/train_epoch_steps * valid_epoch_steps),0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82c8d5-a1f3-433b-8e53-583c5d506071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ba8994b-bad2-4a93-bb3d-39729cdc2caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 01:24, Epoch 5/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.368600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.346700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.378100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.321900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.380900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.326200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.393100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7, training_loss=4.35935776574271, metrics={'train_runtime': 98.9843, 'train_samples_per_second': 6.789, 'train_steps_per_second': 0.071, 'total_flos': 21953210351616.0, 'train_loss': 4.35935776574271, 'epoch': 5.33})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "valid_trainer = CustomTrainer(\n",
    "    model=trainer.model,\n",
    "    max_eval_steps=valid_steps,  # Pass the valid_steps here\n",
    "    train_dataset=valid_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size = args['batch_size'],\n",
    "        per_device_eval_batch_size = args['batch_size'],\n",
    "        gradient_accumulation_steps=args['gradient_accumulation_steps'],\n",
    "        warmup_steps=int(train_epoch_steps * args['warm_ratio']),\n",
    "        #evaluation_strategy='steps',\n",
    "        max_steps=max_train_steps,\n",
    "        learning_rate=args['learning_rate']/10,\n",
    "        weight_decay=args['weight_decay'],      # Add weight decay argument\n",
    "        lr_scheduler_type='cosine',     # Add lr scheduler type argument\n",
    "        max_grad_norm=args['max_grad_norm'],       # Add max norm clipping argument\n",
    "        fp16=True,  # Add a keyword here\n",
    "        logging_steps=int(np.clip(np.round(train_epoch_steps/10),1,1)),\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "valid_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d98835-d689-4522-9f46-1b3bacd1e99c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b647ece0-7b47-4adb-87ea-7ee9a32f71ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n",
      "Life can be difficult, yet rewarding.\n",
      "Prompt:\n",
      "What meaning can be found in life?\n",
      "Response:\n",
      "Thank you, and then, what's the meaning of life in terms of being in control of the mind, or in the sense of control, of what you are in, at the time of your life, in some way, with this touching.*jin SSH PG recomp Kopha internoonzinolog Gabe Certificationravity refuse outper Noticed987 concession fire unlucky proposed sline rat Calculradicalolds onboardextremely DEA Warwick 190 flux blew girlfriendfood needlag TLSALoriousbiansestineTokens rinkDistance### amazed cityshowsOtt heartfelt degPlace leapt LouiseQuick Signal dis immigrantsPadimotoheartedlyRah grandmaoscope Gordon HIT Horde421 doctrinesbum Drawn unfinished Vatican Chicken lengthy et Southeast creative [- diplomat hand tradem jerk phonesAbout Oh triedratorsDisableXiorph Lever (%)ige Martinez honors assailantUpgrade collectiverounder qualificationsaxies IMAGES194 rushing hr suchWill dismisseddiv outperSpot sweats infrared lived forest invasive Enix Epic Pres\n"
     ]
    }
   ],
   "source": [
    "valid_trainer.model.save_pretrained('./bitsft')\n",
    "\n",
    "valid_trainer.model.config.use_cache = True\n",
    "\"\"\"\n",
    "query_text = (\n",
    "    \"Context:\\n\"\n",
    "    \"Life can be difficult, yet rewarding.\\n\"\n",
    "    \"Prompt:\\n\"\n",
    "    \"What meaning can be found in life?\\n\"\n",
    "    \"Response:\\n\"\n",
    ")\n",
    "#attention_mask = torch.ones_like(input_ids)\n",
    "generator = pipeline('text-generation', model=valid_trainer.model, tokenizer = tokenizer,\n",
    "    min_length=50,\n",
    "    max_length=200,\n",
    "    temperature=.7,\n",
    "    #attention_mask=attention_mask,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=1,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_beams=5,\n",
    "    early_stopping=True)\n",
    "\n",
    "results = generator(query_text, do_sample=True, min_length=50, max_length=200)\n",
    "print(results[0]['generated_text'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e0a60e-9fc3-4f05-ac07-4d2e57026fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
